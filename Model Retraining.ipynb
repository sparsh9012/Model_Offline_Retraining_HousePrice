{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 12: Imagine that your model has been in deployed state for a while and has been making predictions. Extend your model deployment application to collect feedback (correct response value) on the predictions made by the model. \n",
    "Outline and demonstrate your model refresh strategy. How will you incorporate the feedback so as to retrain and improve the model with time? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKlearn partial fit (http://scikit-learn.org/stable/modules/scaling_strategies.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn import linear_model\n",
    " \n",
    "# X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "# Y = np.array([1, 1, 2, 2])\n",
    " \n",
    "# clf = linear_model.SGDClassifier()\n",
    "# clf.fit(X, Y) # Train the model for the first time.\n",
    " \n",
    "# X_new = np.array([[-1.2, -0.4], [2, 1.9], [-1, 1], [1, 2]])\n",
    "# Y_new = np.array([2, 1, 1, 2])\n",
    " \n",
    "# clf.partial_fit(X_new, Y_new) # Update the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline (retrain on the entire dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online learning\n",
    "1. Save new training data as you receive it. For example, if you are receiving updated prices of houses on the market, save that information to a database.\n",
    "2. When you have enough new data, test its accuracy against your machine learning model.\n",
    "3. If you see the accuracy of your model degrading over time, use the new data, or a combination of the new data and old training data to build and deploy a new model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-training 1\n",
    "1. Split Available dataset into 3 part {df(80):new(20)} -> {train(80):test(20)}\n",
    "2. Receive New-dataset CSV file (Flask Jinga)\n",
    "3. Train Model using Train-Test & calculate 'Accuracy before Re-training'\n",
    "4. Add New data and re-split into {Train:80, Test:20} & calculate 'Accuracy after Re-training'\n",
    "5. Print Both Accuracy (Flask Jinga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Split Available dataset into 3 part {df(80):new(20)} -> {train(80):test(20)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (18,19,20,21,22,23,24,33,34,35,37,43,45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('DC_Properties.csv')\n",
    "\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "df = data[msk]\n",
    "new = data[~msk]\n",
    "\n",
    "new.to_csv('new.csv', index=False)\n",
    "df.to_csv('old.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oldmodel.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removable_features = ['SOURCE', 'CENSUS_TRACT', 'CENSUS_BLOCK', 'SQUARE', 'USECODE', 'Unnamed: 0', 'X', 'Y']\n",
    "df = df.drop(removable_features, axis=1)\n",
    "\n",
    "df['SALEDATE'] = pd.to_datetime(df['SALEDATE'])\n",
    "df['SALE_YR'] = df['SALEDATE'].dt.year; df['SALE_MONTH'] = df['SALEDATE'].dt.month\n",
    "df = df.drop('SALEDATE', axis=1)\n",
    "df = df.drop('GIS_LAST_MOD_DTTM', axis=1)\n",
    "df = df[~pd.isnull(df['PRICE'])]\n",
    "df = df.drop(df[df.PRICE>2000000].index)\n",
    "\n",
    "df = df.loc[:, df.isnull().sum() < 0.4*df.shape[0]]\n",
    "df = df.drop('ASSESSMENT_SUBNBHD', axis=1)\n",
    "df = df.dropna(how='any')\n",
    "df = df.drop(['LATITUDE','LONGITUDE','ZIPCODE'], axis=1)\n",
    "\n",
    "df = df[df.FIREPLACES<=11]\n",
    "df = df[df.AYB>=1825]\n",
    "df = df[df.EYB>=1900]\n",
    "df = df[df.LANDAREA<=50000]\n",
    "\n",
    "df = pd.concat([df, pd.get_dummies(df[['AC','QUALIFIED','WARD','QUADRANT']])], axis=1)\n",
    "df = df.drop(['AC','QUALIFIED','WARD','QUADRANT'], axis=1)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['HEAT']=le.fit_transform(df['HEAT'])\n",
    "df['ASSESSMENT_NBHD']=le.fit_transform(df['ASSESSMENT_NBHD'])\n",
    "\n",
    "df[\"PRICE\"] = np.log1p(df[\"PRICE\"])\n",
    "df = df.drop(df[df.PRICE<10].index)\n",
    "#y = df['PRICE']; X = df.drop('PRICE', axis=1)\n",
    "\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train = df[msk]\n",
    "y_train = train['PRICE']; X_train = train.drop('PRICE', axis=1)\n",
    "\n",
    "test = df[~msk]\n",
    "test.to_csv('test_data.csv', index=False)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "forest_reg.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "# now you can save it to a file\n",
    "joblib.dump(forest_reg, 'oldmodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Runk in Flask python script'''\n",
    "# predictions = forest_reg.predict(X_test)\n",
    "# from sklearn import metrics\n",
    "# mae_before = metrics.mean_absolute_error(y_test, predictions)\n",
    "# mse_before = metrics.mean_squared_error(y_test, predictions)\n",
    "# rmse_before = np.sqrt(metrics.mean_squared_error(y_test, predictions))\n",
    "# rsq_before = metrics.r2_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new data and re-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_data = pd.read_csv('old.csv')\n",
    "# new_data = pd.read_csv('new.csv')\n",
    "# df = pd.concat([old_data, new_data])\n",
    "\n",
    "# removable_features = ['SOURCE', 'CENSUS_TRACT', 'CENSUS_BLOCK', 'SQUARE', 'USECODE', 'Unnamed: 0', 'X', 'Y']\n",
    "# df = df.drop(removable_features, axis=1)\n",
    "\n",
    "# df['SALEDATE'] = pd.to_datetime(df['SALEDATE'])\n",
    "# df['SALE_YR'] = df['SALEDATE'].dt.year; df['SALE_MONTH'] = df['SALEDATE'].dt.month\n",
    "# df = df.drop('SALEDATE', axis=1)\n",
    "# df = df.drop('GIS_LAST_MOD_DTTM', axis=1)\n",
    "# df = df[~pd.isnull(df['PRICE'])]\n",
    "# df = df.drop(df[df.PRICE>2000000].index)\n",
    "\n",
    "# df = df.loc[:, df.isnull().sum() < 0.4*df.shape[0]]\n",
    "# df = df.drop('ASSESSMENT_SUBNBHD', axis=1)\n",
    "# df = df.dropna(how='any')\n",
    "# df = df.drop(['LATITUDE','LONGITUDE','ZIPCODE'], axis=1)\n",
    "\n",
    "# df = df[df.FIREPLACES<=11]\n",
    "# df = df[df.AYB>=1825]\n",
    "# df = df[df.EYB>=1900]\n",
    "# df = df[df.LANDAREA<=50000]\n",
    "\n",
    "# df = pd.concat([df, pd.get_dummies(df[['AC','QUALIFIED','WARD','QUADRANT']])], axis=1)\n",
    "# df = df.drop(['AC','QUALIFIED','WARD','QUADRANT'], axis=1)\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# df['HEAT']=le.fit_transform(df['HEAT'])\n",
    "# df['ASSESSMENT_NBHD']=le.fit_transform(df['ASSESSMENT_NBHD'])\n",
    "\n",
    "# df[\"PRICE\"] = np.log1p(df[\"PRICE\"])\n",
    "# df = df.drop(df[df.PRICE<10].index)\n",
    "# y = df['PRICE']; X = df.drop('PRICE', axis=1)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20)\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# forest_reg = RandomForestRegressor(random_state=42)\n",
    "# forest_reg.fit(X_train, y_train)\n",
    "# predictions = forest_reg.predict(X_test)\n",
    "\n",
    "# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# from sklearn import metrics\n",
    "# mae_after = metrics.mean_absolute_error(y_test, predictions)\n",
    "# mse_after = metrics.mean_squared_error(y_test, predictions)\n",
    "# rmse_after = np.sqrt(metrics.mean_squared_error(y_test, predictions))\n",
    "# rsq_after = metrics.r2_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (18,19,20,21,22,23,24,33,34,35,37,43,45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "new_data = pd.read_csv('new.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "testY_oldmodel = test_data['PRICE'].values\n",
    "testX_oldmodel = test_data.drop('PRICE', axis=1)\n",
    "\n",
    "old_model = joblib.load('oldmodel.pkl')\n",
    "predictions = old_model.predict(testX_oldmodel)\n",
    "from sklearn import metrics\n",
    "mae_before = metrics.mean_absolute_error(testY_oldmodel, predictions)\n",
    "mse_before = metrics.mean_squared_error(testY_oldmodel, predictions)\n",
    "rmse_before = np.sqrt(metrics.mean_squared_error(testY_oldmodel, predictions))\n",
    "rsq_before = metrics.r2_score(testY_oldmodel, predictions)\n",
    "\n",
    "old_data = pd.read_csv('old.csv')\n",
    "#new_data = pd.read_csv(request.files.get('file')) #From user (through web-interface)\n",
    "df = pd.concat([old_data, new_data])\n",
    "\n",
    "removable_features = ['SOURCE', 'CENSUS_TRACT', 'CENSUS_BLOCK', 'SQUARE', 'USECODE', 'Unnamed: 0', 'X', 'Y']\n",
    "df = df.drop(removable_features, axis=1)\n",
    "\n",
    "df['SALEDATE'] = pd.to_datetime(df['SALEDATE'])\n",
    "df['SALE_YR'] = df['SALEDATE'].dt.year; df['SALE_MONTH'] = df['SALEDATE'].dt.month\n",
    "df = df.drop('SALEDATE', axis=1)\n",
    "df = df.drop('GIS_LAST_MOD_DTTM', axis=1)\n",
    "df = df[~pd.isnull(df['PRICE'])]\n",
    "df = df.drop(df[df.PRICE>2000000].index)\n",
    "\n",
    "df = df.loc[:, df.isnull().sum() < 0.4*df.shape[0]]\n",
    "df = df.drop('ASSESSMENT_SUBNBHD', axis=1)\n",
    "df = df.dropna(how='any')\n",
    "df = df.drop(['LATITUDE','LONGITUDE','ZIPCODE'], axis=1)\n",
    "\n",
    "df = df[df.FIREPLACES<=11]\n",
    "df = df[df.AYB>=1825]\n",
    "df = df[df.EYB>=1900]\n",
    "df = df[df.LANDAREA<=50000]\n",
    "\n",
    "df = pd.concat([df, pd.get_dummies(df[['AC','QUALIFIED','WARD','QUADRANT']])], axis=1)\n",
    "df = df.drop(['AC','QUALIFIED','WARD','QUADRANT'], axis=1)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['HEAT']=le.fit_transform(df['HEAT'])\n",
    "df['ASSESSMENT_NBHD']=le.fit_transform(df['ASSESSMENT_NBHD'])\n",
    "\n",
    "df[\"PRICE\"] = np.log1p(df[\"PRICE\"])\n",
    "df = df.drop(df[df.PRICE<10].index)\n",
    "y = df['PRICE']; X = df.drop('PRICE', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg_new = RandomForestRegressor(random_state=42)\n",
    "forest_reg_new.fit(X_train, y_train)\n",
    "predictions = forest_reg_new.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "mae_after = metrics.mean_absolute_error(y_test, predictions)\n",
    "mse_after = metrics.mean_squared_error(y_test, predictions)\n",
    "rmse_after = np.sqrt(metrics.mean_squared_error(y_test, predictions))\n",
    "rsq_after = metrics.r2_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
